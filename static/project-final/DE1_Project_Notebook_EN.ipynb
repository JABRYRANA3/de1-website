{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157db7bd",
   "metadata": {},
   "source": [
    "# DE1 — Final Project Notebook\n",
    "> Author : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n",
    "---\n",
    "\n",
    "This is the primary executable artifact. Fill config, run baseline, then optimized pipeline, and record evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af906c6",
   "metadata": {},
   "source": [
    "## 0. Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc92bdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/04 14:54:58 WARN Utils: Your hostname, Rana, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/01/04 14:54:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/04 14:54:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'paths': {'raw_tsv_glob': 'data/raw/*.tsv',\n",
       "  'outputs_root': 'outputs/project',\n",
       "  'bronze': 'outputs/project/bronze',\n",
       "  'silver': 'outputs/project/silver',\n",
       "  'gold': 'outputs/project/gold',\n",
       "  'proof': 'proof',\n",
       "  'metrics_log': 'project_metrics_log.csv'},\n",
       " 'layout': {'target_file_size_mb': 128, 'num_partitions': 200},\n",
       " 'queries': {'q1': {'name': 'Top pages by total clicks',\n",
       "   'sql': 'SELECT\\n  source_page AS page,\\n  SUM(click_count) AS total_clicks\\nFROM silver\\nGROUP BY source_page\\nORDER BY total_clicks DESC\\nLIMIT 100\\n'},\n",
       "  'q2': {'name': 'Top targets per source',\n",
       "   'sql': 'SELECT\\n  source_page,\\n  target_page,\\n  SUM(click_count) AS total_clicks\\nFROM silver\\nGROUP BY source_page, target_page\\nORDER BY total_clicks DESC\\nLIMIT 100\\n'},\n",
       "  'q3': {'name': 'Top link types',\n",
       "   'sql': 'SELECT\\n  link_type,\\n  COUNT(*) AS edges,\\n  SUM(click_count) AS total_clicks\\nFROM silver\\nGROUP BY link_type\\nORDER BY total_clicks DESC\\nLIMIT 100\\n'}}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "#le YAML est dans le même dossier que le notebook\n",
    "with open(\"de1_project_config.yml\") as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"de1-project\").getOrCreate()\n",
    "\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc0dd22d-5ba4-4a78-96aa-d220a17cff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")   # ou \"WARN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21551f0a",
   "metadata": {},
   "source": [
    "## 1. Bronze — landing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75988650-bdc4-4467-b341-680272d6a053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze written to: outputs/project/bronze\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "raw_glob = CFG[\"paths\"][\"raw_tsv_glob\"]   \n",
    "bronze   = CFG[\"paths\"][\"bronze\"]\n",
    "proof    = CFG[\"paths\"][\"proof\"]\n",
    "\n",
    "df_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"false\")   \n",
    "    .option(\"sep\", \"\\t\")         \n",
    "    .csv(raw_glob)\n",
    ")\n",
    "\n",
    "df_raw.write.mode(\"overwrite\").csv(bronze)  \n",
    "print(\"Bronze written to:\", bronze)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a13e8",
   "metadata": {},
   "source": [
    "## 2. Silver — cleaning and typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c46f60-70c2-49ba-9edc-e5a4a5f1d47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:================================>                         (9 + 7) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46.504s][warning][gc,alloc] Executor task launch worker for task 12.0 in stage 10.0 (TID 79): Retried waiting for GCLocker too often allocating 524290 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver written: outputs/project/silver, rows: 6,072,131\n"
     ]
    }
   ],
   "source": [
    "# 2. Silver — cleaning and typing (ADAPTÉ AU DATASET)\n",
    "\n",
    "silver = CFG[\"paths\"][\"silver\"]\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_silver = (\n",
    "    df_raw\n",
    "    .withColumnRenamed(\"_c0\", \"source_page\")\n",
    "    .withColumnRenamed(\"_c1\", \"target_page\")\n",
    "    .withColumnRenamed(\"_c2\", \"link_type\")\n",
    "    .withColumnRenamed(\"_c3\", \"click_count\")\n",
    "    .withColumn(\"click_count\", F.col(\"click_count\").cast(\"int\"))\n",
    "    .filter(F.col(\"click_count\").isNotNull())\n",
    "    .filter(F.col(\"click_count\") >= 0)\n",
    "    .filter(F.length(F.col(\"source_page\")) > 0)\n",
    "    .filter(F.length(F.col(\"target_page\")) > 0)\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "silver_count = df_silver.count()\n",
    "df_silver.write.mode(\"overwrite\").parquet(silver)\n",
    "\n",
    "print(f\"Silver written: {silver}, rows: {silver_count:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5ba24",
   "metadata": {},
   "source": [
    "## 3. Gold — analytics tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b64c37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 written, rows: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                       (0 + 16) / 17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91.636s][warning][gc,alloc] Executor task launch worker for task 13.0 in stage 27.0 (TID 215): Retried waiting for GCLocker too often allocating 1048578 words\n",
      "[91.674s][warning][gc,alloc] Executor task launch worker for task 6.0 in stage 27.0 (TID 208): Retried waiting for GCLocker too often allocating 1048578 words\n",
      "[92.142s][warning][gc,alloc] Executor task launch worker for task 1.0 in stage 27.0 (TID 203): Retried waiting for GCLocker too often allocating 1048578 words\n",
      "[92.152s][warning][gc,alloc] Executor task launch worker for task 12.0 in stage 27.0 (TID 214): Retried waiting for GCLocker too often allocating 1048578 words\n",
      "[92.244s][warning][gc,alloc] Executor task launch worker for task 7.0 in stage 27.0 (TID 209): Retried waiting for GCLocker too often allocating 1048578 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                                                       (0 + 16) / 17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107.936s][warning][gc,alloc] Executor task launch worker for task 2.0 in stage 34.0 (TID 255): Retried waiting for GCLocker too often allocating 1048578 words\n",
      "[108.978s][warning][gc,alloc] Executor task launch worker for task 7.0 in stage 34.0 (TID 260): Retried waiting for GCLocker too often allocating 1048578 words\n",
      "[109.032s][warning][gc,alloc] Executor task launch worker for task 9.0 in stage 34.0 (TID 262): Retried waiting for GCLocker too often allocating 1048578 words\n",
      "[109.033s][warning][gc,alloc] Executor task launch worker for task 7.0 in stage 34.0 (TID 260): Retried waiting for GCLocker too often allocating 516091 words\n",
      "[109.117s][warning][gc,alloc] Executor task launch worker for task 15.0 in stage 34.0 (TID 268): Retried waiting for GCLocker too often allocating 524290 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:==============================>                          (9 + 8) / 17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112.869s][warning][gc,alloc] Executor task launch worker for task 5.0 in stage 37.0 (TID 275): Retried waiting for GCLocker too often allocating 524290 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 written, rows: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 written, rows: 3\n",
      "Gold written: outputs/project/gold\n"
     ]
    }
   ],
   "source": [
    "# 3. Gold — analytics tables\n",
    "\n",
    "gold = CFG[\"paths\"][\"gold\"]\n",
    "queries = CFG[\"queries\"]\n",
    "\n",
    "import pathlib\n",
    "pathlib.Path(gold).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Register silver as SQL table\n",
    "df_silver.createOrReplaceTempView(\"silver\")\n",
    "\n",
    "# Q1 — Top destination pages\n",
    "df_q1 = spark.sql(queries[\"q1\"][\"sql\"])\n",
    "q1_count = df_q1.count()\n",
    "df_q1.write.mode(\"overwrite\").parquet(f\"{gold}/q1_top_pages\")\n",
    "print(f\"Q1 written, rows: {q1_count:,}\")\n",
    "\n",
    "# Q2 — Top transitions\n",
    "df_q2 = spark.sql(queries[\"q2\"][\"sql\"])\n",
    "q2_count = df_q2.count()\n",
    "df_q2.write.mode(\"overwrite\").parquet(f\"{gold}/q2_top_transitions\")\n",
    "print(f\"Q2 written, rows: {q2_count:,}\")\n",
    "\n",
    "# Q3 — High traffic links\n",
    "df_q3 = spark.sql(queries[\"q3\"][\"sql\"])\n",
    "q3_count = df_q3.count()\n",
    "df_q3.write.mode(\"overwrite\").parquet(f\"{gold}/q3_high_traffic\")\n",
    "print(f\"Q3 written, rows: {q3_count:,}\")\n",
    "\n",
    "print(\"Gold written:\", gold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3267b1cb-846d-469e-83d9-16ade8d39596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN Q1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"RUN Q1\")\n",
    "df_q1 = spark.sql(queries[\"q1\"][\"sql\"])\n",
    "df_q1.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006b1c71-0f63-433a-814d-238e8d7ca430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN Q2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                       (0 + 16) / 17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[159.929s][warning][gc,alloc] Executor task launch worker for task 10.0 in stage 60.0 (TID 433): Retried waiting for GCLocker too often allocating 1048578 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"RUN Q2\")\n",
    "df_q2 = spark.sql(queries[\"q2\"][\"sql\"])\n",
    "df_q2.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bfc960d-615a-4d45-a82e-9be44cb7849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN Q3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"RUN Q3\")\n",
    "df_q3 = spark.sql(queries[\"q3\"][\"sql\"])\n",
    "df_q3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0338d7",
   "metadata": {},
   "source": [
    "## 4. Baseline plans and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1883c98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: proof/baseline_q1_plan.txt\n",
      "Saved: proof/baseline_q2_plan.txt\n",
      "Saved: proof/baseline_q3_plan.txt\n",
      "Saved baseline plans. Now open Spark UI \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Baseline plans and metrics (save plans to proof/)\n",
    "import datetime\n",
    "import pathlib\n",
    "\n",
    "proof = CFG[\"paths\"][\"proof\"]\n",
    "pathlib.Path(proof).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_plan(df, name):\n",
    "    plan = df._jdf.queryExecution().executedPlan().toString()\n",
    "    out = f\"{proof}/baseline_{name}_plan.txt\"\n",
    "    with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(datetime.datetime.now()) + \"\\n\")\n",
    "        f.write(plan)\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "# IMPORTANT: on utilise les df déjà calculés en section 3 (gold)\n",
    "save_plan(df_q1, \"q1\")\n",
    "save_plan(df_q2, \"q2\")\n",
    "save_plan(df_q3, \"q3\")\n",
    "\n",
    "print(\"Saved baseline plans. Now open Spark UI \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6206648f",
   "metadata": {},
   "source": [
    "## 5. Optimization — layout and joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aebf893-1726-42c8-a89e-db321c9754cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized silver written: outputs/project/silver_optimized\n",
      "Saved optimized plans. Now run Q1-Q3 (optimized) and record Spark UI metrics.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import datetime, pathlib\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver = CFG[\"paths\"][\"silver\"]\n",
    "proof  = CFG[\"paths\"][\"proof\"]\n",
    "gold   = CFG[\"paths\"][\"gold\"]\n",
    "queries = CFG[\"queries\"]\n",
    "layout = CFG.get(\"layout\", {})\n",
    "\n",
    "pathlib.Path(proof).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(gold).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "df_silver_reloaded = spark.read.parquet(silver)\n",
    "\n",
    "\n",
    "num_partitions = int(layout.get(\"num_partitions\", 200))\n",
    "\n",
    "df_silver_opt = (df_silver_reloaded\n",
    "    .repartition(num_partitions)\n",
    "    .sortWithinPartitions(F.desc(\"click_count\"))\n",
    ")\n",
    "\n",
    "silver_opt = f\"{silver}_optimized\"\n",
    "df_silver_opt.write.mode(\"overwrite\").parquet(silver_opt)\n",
    "print(f\"Optimized silver written: {silver_opt}\")\n",
    "\n",
    "\n",
    "df_silver_opt.createOrReplaceTempView(\"silver\")\n",
    "\n",
    "\n",
    "df_q1_opt = spark.sql(queries[\"q1\"][\"sql\"])\n",
    "plan_q1_opt = df_q1_opt._jdf.queryExecution().executedPlan().toString()\n",
    "with open(f\"{proof}/optimized_q1_plan.txt\", \"w\") as f:\n",
    "    f.write(str(datetime.datetime.now()) + \"\\n\")\n",
    "    f.write(f\"Optimization: repartition({num_partitions}) + sortWithinPartitions(click_count desc)\\n\")\n",
    "    f.write(plan_q1_opt)\n",
    "\n",
    "df_q2_opt = spark.sql(queries[\"q2\"][\"sql\"])\n",
    "plan_q2_opt = df_q2_opt._jdf.queryExecution().executedPlan().toString()\n",
    "with open(f\"{proof}/optimized_q2_plan.txt\", \"w\") as f:\n",
    "    f.write(str(datetime.datetime.now()) + \"\\n\")\n",
    "    f.write(f\"Optimization: repartition({num_partitions}) + sortWithinPartitions(click_count desc)\\n\")\n",
    "    f.write(plan_q2_opt)\n",
    "\n",
    "df_q3_opt = spark.sql(queries[\"q3\"][\"sql\"])\n",
    "plan_q3_opt = df_q3_opt._jdf.queryExecution().executedPlan().toString()\n",
    "with open(f\"{proof}/optimized_q3_plan.txt\", \"w\") as f:\n",
    "    f.write(str(datetime.datetime.now()) + \"\\n\")\n",
    "    f.write(f\"Optimization: repartition({num_partitions}) + sortWithinPartitions(click_count desc)\\n\")\n",
    "    f.write(plan_q3_opt)\n",
    "\n",
    "print(\"Saved optimized plans. Now run Q1-Q3 (optimized) and record Spark UI metrics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb68064c-3037-4e3e-a6b9-3028a31cb05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN Q1 OPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN Q2 OPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN Q3 OPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"RUN Q1 OPT\"); spark.sql(queries[\"q1\"][\"sql\"]).count()\n",
    "print(\"RUN Q2 OPT\"); spark.sql(queries[\"q2\"][\"sql\"]).count()\n",
    "print(\"RUN Q3 OPT\"); spark.sql(queries[\"q3\"][\"sql\"]).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11956a65",
   "metadata": {},
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42bfb7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824b6b4-b042-4227-a3f2-dc19e89d12f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
