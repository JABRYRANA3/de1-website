{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be00c820",
   "metadata": {},
   "source": [
    "# DE1 — Lab 2: PostgreSQL → Star Schema ETL\n",
    "> Author : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n",
    "---\n",
    "\n",
    "\n",
    "Execute all cells. Attach evidence and fill metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47183010",
   "metadata": {},
   "source": [
    "## 0. Setup and schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df4f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/21 20:31:18 WARN Utils: Your hostname, Rana, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/21 20:31:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/21 20:31:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "spark = SparkSession.builder.appName(\"de1-lab2\").getOrCreate()\n",
    "base = \"\"\n",
    "# Explicit schemas\n",
    "customers_schema = T.StructType([\n",
    "    T.StructField(\"customer_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"name\", T.StringType(), True),\n",
    "    T.StructField(\"email\", T.StringType(), True),\n",
    "    T.StructField(\"created_at\", T.TimestampType(), True),\n",
    "])\n",
    "brands_schema = T.StructType([\n",
    "    T.StructField(\"brand_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"brand_name\", T.StringType(), True),\n",
    "])\n",
    "categories_schema = T.StructType([\n",
    "    T.StructField(\"category_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"category_name\", T.StringType(), True),\n",
    "])\n",
    "products_schema = T.StructType([\n",
    "    T.StructField(\"product_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"product_name\", T.StringType(), True),\n",
    "    T.StructField(\"brand_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"category_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"price\", T.DoubleType(), True),\n",
    "])\n",
    "orders_schema = T.StructType([\n",
    "    T.StructField(\"order_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"customer_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"order_date\", T.TimestampType(), True),\n",
    "])\n",
    "order_items_schema = T.StructType([\n",
    "    T.StructField(\"order_item_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"order_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"product_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"quantity\", T.IntegerType(), True),\n",
    "    T.StructField(\"unit_price\", T.DoubleType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c59303e",
   "metadata": {},
   "source": [
    "## 1. Ingest operational tables (from CSV exports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f7254e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers 24\n",
      "brands 8\n",
      "categories 9\n",
      "products 60\n",
      "orders 220\n",
      "order_items 638\n"
     ]
    }
   ],
   "source": [
    "customers = spark.read.schema(customers_schema).option(\"header\",\"true\").csv(base+\"lab2_customers.csv\")\n",
    "brands = spark.read.schema(brands_schema).option(\"header\",\"true\").csv(base+\"lab2_brands.csv\")\n",
    "categories = spark.read.schema(categories_schema).option(\"header\",\"true\").csv(base+\"lab2_categories.csv\")\n",
    "products = spark.read.schema(products_schema).option(\"header\",\"true\").csv(base+\"lab2_products.csv\")\n",
    "orders = spark.read.schema(orders_schema).option(\"header\",\"true\").csv(base+\"lab2_orders.csv\")\n",
    "order_items = spark.read.schema(order_items_schema).option(\"header\",\"true\").csv(base+\"lab2_order_items.csv\")\n",
    "\n",
    "for name, df in [(\"customers\",customers),(\"brands\",brands),(\"categories\",categories),(\"products\",products),(\"orders\",orders),(\"order_items\",order_items)]:\n",
    "    print(name, df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba3757",
   "metadata": {},
   "source": [
    "### Evidence: ingestion plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa4f780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (11)\n",
      "+- HashAggregate (10)\n",
      "   +- Exchange (9)\n",
      "      +- HashAggregate (8)\n",
      "         +- Project (7)\n",
      "            +- BroadcastHashJoin Inner BuildLeft (6)\n",
      "               :- BroadcastExchange (3)\n",
      "               :  +- Filter (2)\n",
      "               :     +- Scan csv  (1)\n",
      "               +- Filter (5)\n",
      "                  +- Scan csv  (4)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [1]: [order_id#13]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab02/lab2_orders.csv]\n",
      "PushedFilters: [IsNotNull(order_id)]\n",
      "ReadSchema: struct<order_id:int>\n",
      "\n",
      "(2) Filter\n",
      "Input [1]: [order_id#13]\n",
      "Condition : isnotnull(order_id#13)\n",
      "\n",
      "(3) BroadcastExchange\n",
      "Input [1]: [order_id#13]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=221]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [1]: [order_id#17]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab02/lab2_order_items.csv]\n",
      "PushedFilters: [IsNotNull(order_id)]\n",
      "ReadSchema: struct<order_id:int>\n",
      "\n",
      "(5) Filter\n",
      "Input [1]: [order_id#17]\n",
      "Condition : isnotnull(order_id#17)\n",
      "\n",
      "(6) BroadcastHashJoin\n",
      "Left keys [1]: [order_id#13]\n",
      "Right keys [1]: [order_id#17]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(7) Project\n",
      "Output [1]: [order_id#13]\n",
      "Input [2]: [order_id#13, order_id#17]\n",
      "\n",
      "(8) HashAggregate\n",
      "Input [1]: [order_id#13]\n",
      "Keys [1]: [order_id#13]\n",
      "Functions: []\n",
      "Aggregate Attributes: []\n",
      "Results [1]: [order_id#13]\n",
      "\n",
      "(9) Exchange\n",
      "Input [1]: [order_id#13]\n",
      "Arguments: hashpartitioning(order_id#13, 200), ENSURE_REQUIREMENTS, [plan_id=226]\n",
      "\n",
      "(10) HashAggregate\n",
      "Input [1]: [order_id#13]\n",
      "Keys [1]: [order_id#13]\n",
      "Functions: []\n",
      "Aggregate Attributes: []\n",
      "Results [1]: [order_id#13]\n",
      "\n",
      "(11) AdaptiveSparkPlan\n",
      "Output [1]: [order_id#13]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Saved proof/plan_ingest.txt\n"
     ]
    }
   ],
   "source": [
    "ingest = orders.join(order_items, \"order_id\").select(\"order_id\").distinct()\n",
    "ingest.explain(\"formatted\")\n",
    "from datetime import datetime as _dt\n",
    "import pathlib\n",
    "pathlib.Path(\"proof\").mkdir(exist_ok=True)\n",
    "with open(\"proof/plan_ingest.txt\",\"w\") as f:\n",
    "    f.write(str(_dt.now())+\"\\n\")\n",
    "    f.write(ingest._jdf.queryExecution().executedPlan().toString())\n",
    "print(\"Saved proof/plan_ingest.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fbf42e",
   "metadata": {},
   "source": [
    "## 2. Surrogate key function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ebc357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk(cols):\n",
    "    # stable 64-bit positive surrogate key from natural keys\n",
    "    return F.abs(F.xxhash64(*[F.col(c) for c in cols]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c0501",
   "metadata": {},
   "source": [
    "## 3. Build dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3846b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_customer = customers.select(\n",
    "    sk([\"customer_id\"]).alias(\"customer_sk\"),\n",
    "    \"customer_id\",\"name\",\"email\",\"created_at\"\n",
    ")\n",
    "\n",
    "dim_brand = brands.select(\n",
    "    sk([\"brand_id\"]).alias(\"brand_sk\"),\n",
    "    \"brand_id\",\"brand_name\"\n",
    ")\n",
    "\n",
    "dim_category = categories.select(\n",
    "    sk([\"category_id\"]).alias(\"category_sk\"),\n",
    "    \"category_id\",\"category_name\"\n",
    ")\n",
    "\n",
    "dim_product = products.select(\n",
    "    sk([\"product_id\"]).alias(\"product_sk\"),\n",
    "    \"product_id\",\"product_name\",\n",
    "    sk([\"brand_id\"]).alias(\"brand_sk\"),\n",
    "    sk([\"category_id\"]).alias(\"category_sk\"),\n",
    "    \"price\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fde5a97-0c32-4dc2-9b97-1fd79a80ac2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_customer: 24\n",
      "dim_brand: 8\n",
      "dim_category: 9\n",
      "dim_product: 60\n"
     ]
    }
   ],
   "source": [
    "print(\"dim_customer:\", dim_customer.count())\n",
    "print(\"dim_brand:\", dim_brand.count())\n",
    "print(\"dim_category:\", dim_category.count())\n",
    "print(\"dim_product:\", dim_product.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf36d6",
   "metadata": {},
   "source": [
    "## 4. Build date dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d3288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window as W\n",
    "dates = orders.select(F.to_date(\"order_date\").alias(\"date\")).distinct()\n",
    "dim_date = dates.select(\n",
    "    sk([\"date\"]).alias(\"date_sk\"),\n",
    "    F.col(\"date\"),\n",
    "    F.year(\"date\").alias(\"year\"),\n",
    "    F.month(\"date\").alias(\"month\"),\n",
    "    F.dayofmonth(\"date\").alias(\"day\"),\n",
    "    F.date_format(\"date\",\"E\").alias(\"dow\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b84175",
   "metadata": {},
   "source": [
    "## 5. Build fact_sales with broadcast joins where appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d06e675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (19)\n",
      "+- Project (18)\n",
      "   +- Project (17)\n",
      "      +- BroadcastHashJoin Inner BuildRight (16)\n",
      "         :- Project (12)\n",
      "         :  +- BroadcastHashJoin Inner BuildRight (11)\n",
      "         :     :- Project (7)\n",
      "         :     :  +- BroadcastHashJoin Inner BuildRight (6)\n",
      "         :     :     :- Filter (2)\n",
      "         :     :     :  +- Scan csv  (1)\n",
      "         :     :     +- BroadcastExchange (5)\n",
      "         :     :        +- Filter (4)\n",
      "         :     :           +- Scan csv  (3)\n",
      "         :     +- BroadcastExchange (10)\n",
      "         :        +- Filter (9)\n",
      "         :           +- Scan csv  (8)\n",
      "         +- BroadcastExchange (15)\n",
      "            +- Filter (14)\n",
      "               +- Scan csv  (13)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [4]: [order_id#17, product_id#18, quantity#19, unit_price#20]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab02/lab2_order_items.csv]\n",
      "PushedFilters: [IsNotNull(product_id), IsNotNull(order_id)]\n",
      "ReadSchema: struct<order_id:int,product_id:int,quantity:int,unit_price:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [4]: [order_id#17, product_id#18, quantity#19, unit_price#20]\n",
      "Condition : (isnotnull(product_id#18) AND isnotnull(order_id#17))\n",
      "\n",
      "(3) Scan csv \n",
      "Output [1]: [product_id#8]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab02/lab2_products.csv]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<product_id:int>\n",
      "\n",
      "(4) Filter\n",
      "Input [1]: [product_id#8]\n",
      "Condition : isnotnull(product_id#8)\n",
      "\n",
      "(5) BroadcastExchange\n",
      "Input [1]: [product_id#8]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=419]\n",
      "\n",
      "(6) BroadcastHashJoin\n",
      "Left keys [1]: [product_id#18]\n",
      "Right keys [1]: [product_id#8]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(7) Project\n",
      "Output [4]: [order_id#17, product_id#18, quantity#19, unit_price#20]\n",
      "Input [5]: [order_id#17, product_id#18, quantity#19, unit_price#20, product_id#8]\n",
      "\n",
      "(8) Scan csv \n",
      "Output [3]: [order_id#13, customer_id#14, order_date#15]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab02/lab2_orders.csv]\n",
      "PushedFilters: [IsNotNull(order_id), IsNotNull(customer_id)]\n",
      "ReadSchema: struct<order_id:int,customer_id:int,order_date:timestamp>\n",
      "\n",
      "(9) Filter\n",
      "Input [3]: [order_id#13, customer_id#14, order_date#15]\n",
      "Condition : (isnotnull(order_id#13) AND isnotnull(customer_id#14))\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [3]: [order_id#13, customer_id#14, order_date#15]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=423]\n",
      "\n",
      "(11) BroadcastHashJoin\n",
      "Left keys [1]: [order_id#17]\n",
      "Right keys [1]: [order_id#13]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [6]: [product_id#18, quantity#19, unit_price#20, order_id#13, customer_id#14, order_date#15]\n",
      "Input [7]: [order_id#17, product_id#18, quantity#19, unit_price#20, order_id#13, customer_id#14, order_date#15]\n",
      "\n",
      "(13) Scan csv \n",
      "Output [1]: [customer_id#0]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab02/lab2_customers.csv]\n",
      "PushedFilters: [IsNotNull(customer_id)]\n",
      "ReadSchema: struct<customer_id:int>\n",
      "\n",
      "(14) Filter\n",
      "Input [1]: [customer_id#0]\n",
      "Condition : isnotnull(customer_id#0)\n",
      "\n",
      "(15) BroadcastExchange\n",
      "Input [1]: [customer_id#0]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=427]\n",
      "\n",
      "(16) BroadcastHashJoin\n",
      "Left keys [1]: [customer_id#14]\n",
      "Right keys [1]: [customer_id#0]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(17) Project\n",
      "Output [6]: [order_id#13, customer_id#14, product_id#18, cast(order_date#15 as date) AS date#124, quantity#19, unit_price#20]\n",
      "Input [7]: [product_id#18, quantity#19, unit_price#20, order_id#13, customer_id#14, order_date#15, customer_id#0]\n",
      "\n",
      "(18) Project\n",
      "Output [9]: [order_id#13, abs(xxhash64(date#124, 42)) AS date_sk#127L, abs(xxhash64(customer_id#14, 42)) AS customer_sk#128L, abs(xxhash64(product_id#18, 42)) AS product_sk#129L, quantity#19, unit_price#20, (cast(quantity#19 as double) * unit_price#20) AS subtotal#132, year(date#124) AS year#133, month(date#124) AS month#134]\n",
      "Input [6]: [order_id#13, customer_id#14, product_id#18, date#124, quantity#19, unit_price#20]\n",
      "\n",
      "(19) AdaptiveSparkPlan\n",
      "Output [9]: [order_id#13, date_sk#127L, customer_sk#128L, product_sk#129L, quantity#19, unit_price#20, subtotal#132, year#133, month#134]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Saved proof/plan_fact_join.txt\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- Safety: define sk if not already defined ---\n",
    "# (If sk already exists, you can skip this cell)\n",
    "def sk(cols):\n",
    "    # stable surrogate key from natural keys\n",
    "    return F.abs(F.xxhash64(*[F.col(c) for c in cols]))\n",
    "\n",
    "# --- Aliases ---\n",
    "oi = order_items.alias(\"oi\")\n",
    "p  = products.alias(\"p\")\n",
    "o  = orders.alias(\"o\")\n",
    "c  = customers.alias(\"c\")\n",
    "\n",
    "# --- Build df_fact with JOINs (make sure join keys match your columns) ---\n",
    "df_fact = (\n",
    "    oi\n",
    "    .join(p, F.col(\"oi.product_id\") == F.col(\"p.product_id\"), \"inner\")\n",
    "    .join(o, F.col(\"oi.order_id\") == F.col(\"o.order_id\"), \"inner\")\n",
    "    .join(c, F.col(\"o.customer_id\") == F.col(\"c.customer_id\"), \"inner\")\n",
    ")\n",
    "\n",
    "# --- IMPORTANT: select ONLY the columns we need, and make names NON-AMBIGUOUS ---\n",
    "# Choose ONE product_id (here: order_items product_id), and ONE customer_id (from orders).\n",
    "df_fact = df_fact.select(\n",
    "    F.col(\"o.order_id\").alias(\"order_id\"),\n",
    "    F.col(\"o.customer_id\").alias(\"customer_id\"),\n",
    "    F.col(\"oi.product_id\").alias(\"product_id\"),\n",
    "    F.to_date(F.col(\"o.order_date\")).alias(\"date\"),\n",
    "    F.col(\"oi.quantity\").alias(\"quantity\"),\n",
    "    F.col(\"oi.unit_price\").alias(\"unit_price\"),\n",
    ")\n",
    "\n",
    "# --- Attach surrogate keys + metrics columns ---\n",
    "df_fact = (\n",
    "    df_fact\n",
    "    .withColumn(\"date_sk\", sk([\"date\"]))\n",
    "    .withColumn(\"customer_sk\", sk([\"customer_id\"]))\n",
    "    .withColumn(\"product_sk\", sk([\"product_id\"]))\n",
    "    .withColumn(\"quantity\", F.col(\"quantity\").cast(\"int\"))\n",
    "    .withColumn(\"unit_price\", F.col(\"unit_price\").cast(\"double\"))\n",
    "    .withColumn(\"subtotal\", F.col(\"quantity\") * F.col(\"unit_price\"))\n",
    "    .withColumn(\"year\", F.year(\"date\"))\n",
    "    .withColumn(\"month\", F.month(\"date\"))\n",
    "    .select(\n",
    "        \"order_id\",\"date_sk\",\"customer_sk\",\"product_sk\",\n",
    "        \"quantity\",\"unit_price\",\"subtotal\",\"year\",\"month\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Explain + save plan (required evidence) ---\n",
    "df_fact.explain(\"formatted\")\n",
    "\n",
    "from datetime import datetime as _dt\n",
    "with open(\"proof/plan_fact_join.txt\", \"w\") as f:\n",
    "    f.write(str(_dt.now()) + \"\\n\")\n",
    "    f.write(df_fact._jdf.queryExecution().executedPlan().toString())\n",
    "\n",
    "print(\"Saved proof/plan_fact_join.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff5279a2-ead1-48ff-be46-5298bc8034b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "638"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ccb512",
   "metadata": {},
   "source": [
    "## 6. Write Parquet outputs (partitioned by year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c6b7265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet written under outputs/lab2/\n"
     ]
    }
   ],
   "source": [
    "base_out = \"outputs/lab2\"\n",
    "(dim_customer.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_customer\"))\n",
    "(dim_brand.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_brand\"))\n",
    "(dim_category.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_category\"))\n",
    "(dim_product.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_product\"))\n",
    "(dim_date.write.mode(\"overwrite\").parquet(f\"{base_out}/dim_date\"))\n",
    "(df_fact.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").parquet(f\"{base_out}/fact_sales\"))\n",
    "print(\"Parquet written under outputs/lab2/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d752b2",
   "metadata": {},
   "source": [
    "## 7. Plan comparison: projection and layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0087b6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (16)\n",
      "+- HashAggregate (15)\n",
      "   +- Exchange (14)\n",
      "      +- HashAggregate (13)\n",
      "         +- Project (12)\n",
      "            +- BroadcastHashJoin Inner BuildRight (11)\n",
      "               :- Project (7)\n",
      "               :  +- BroadcastHashJoin Inner BuildLeft (6)\n",
      "               :     :- BroadcastExchange (3)\n",
      "               :     :  +- Filter (2)\n",
      "               :     :     +- Scan csv  (1)\n",
      "               :     +- Filter (5)\n",
      "               :        +- Scan csv  (4)\n",
      "               +- BroadcastExchange (10)\n",
      "                  +- Filter (9)\n",
      "                     +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [order_id#13, order_date#15]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab02/lab2_orders.csv]\n",
      "PushedFilters: [IsNotNull(order_id)]\n",
      "ReadSchema: struct<order_id:int,order_date:timestamp>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [order_id#13, order_date#15]\n",
      "Condition : isnotnull(order_id#13)\n",
      "\n",
      "(3) BroadcastExchange\n",
      "Input [2]: [order_id#13, order_date#15]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1240]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [3]: [order_id#17, product_id#18, quantity#19]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab02/lab2_order_items.csv]\n",
      "PushedFilters: [IsNotNull(order_id), IsNotNull(product_id)]\n",
      "ReadSchema: struct<order_id:int,product_id:int,quantity:int>\n",
      "\n",
      "(5) Filter\n",
      "Input [3]: [order_id#17, product_id#18, quantity#19]\n",
      "Condition : (isnotnull(order_id#17) AND isnotnull(product_id#18))\n",
      "\n",
      "(6) BroadcastHashJoin\n",
      "Left keys [1]: [order_id#13]\n",
      "Right keys [1]: [order_id#17]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(7) Project\n",
      "Output [3]: [order_date#15, product_id#18, quantity#19]\n",
      "Input [5]: [order_id#13, order_date#15, order_id#17, product_id#18, quantity#19]\n",
      "\n",
      "(8) Scan csv \n",
      "Output [2]: [product_id#8, price#12]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab02/lab2_products.csv]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<product_id:int,price:double>\n",
      "\n",
      "(9) Filter\n",
      "Input [2]: [product_id#8, price#12]\n",
      "Condition : isnotnull(product_id#8)\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [2]: [product_id#8, price#12]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1244]\n",
      "\n",
      "(11) BroadcastHashJoin\n",
      "Left keys [1]: [product_id#18]\n",
      "Right keys [1]: [product_id#8]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [3]: [quantity#19, price#12, cast(order_date#15 as date) AS _groupingexpression#191]\n",
      "Input [5]: [order_date#15, product_id#18, quantity#19, product_id#8, price#12]\n",
      "\n",
      "(13) HashAggregate\n",
      "Input [3]: [quantity#19, price#12, _groupingexpression#191]\n",
      "Keys [1]: [_groupingexpression#191]\n",
      "Functions [1]: [partial_sum((cast(quantity#19 as double) * price#12))]\n",
      "Aggregate Attributes [1]: [sum#192]\n",
      "Results [2]: [_groupingexpression#191, sum#193]\n",
      "\n",
      "(14) Exchange\n",
      "Input [2]: [_groupingexpression#191, sum#193]\n",
      "Arguments: hashpartitioning(_groupingexpression#191, 200), ENSURE_REQUIREMENTS, [plan_id=1249]\n",
      "\n",
      "(15) HashAggregate\n",
      "Input [2]: [_groupingexpression#191, sum#193]\n",
      "Keys [1]: [_groupingexpression#191]\n",
      "Functions [1]: [sum((cast(quantity#19 as double) * price#12))]\n",
      "Aggregate Attributes [1]: [sum((cast(quantity#19 as double) * price#12))#190]\n",
      "Results [2]: [_groupingexpression#191 AS d#177, sum((cast(quantity#19 as double) * price#12))#190 AS gmv#178]\n",
      "\n",
      "(16) AdaptiveSparkPlan\n",
      "Output [2]: [d#177, gmv#178]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case A: join and then project\n",
    "a = (orders.join(order_items, \"order_id\")\n",
    "            .join(products, \"product_id\")\n",
    "            .groupBy(F.to_date(\"order_date\").alias(\"d\"))\n",
    "            .agg(F.sum(F.col(\"quantity\")*F.col(\"price\")).alias(\"gmv\")))\n",
    "a.explain(\"formatted\")\n",
    "_ = a.count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7860ad7d-7587-4f70-bb56-1ab77ed5f13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (16)\n",
      "+- HashAggregate (15)\n",
      "   +- Exchange (14)\n",
      "      +- HashAggregate (13)\n",
      "         +- Project (12)\n",
      "            +- BroadcastHashJoin Inner BuildRight (11)\n",
      "               :- Project (7)\n",
      "               :  +- BroadcastHashJoin Inner BuildLeft (6)\n",
      "               :     :- BroadcastExchange (3)\n",
      "               :     :  +- Filter (2)\n",
      "               :     :     +- Scan csv  (1)\n",
      "               :     +- Filter (5)\n",
      "               :        +- Scan csv  (4)\n",
      "               +- BroadcastExchange (10)\n",
      "                  +- Filter (9)\n",
      "                     +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [order_id#13, order_date#15]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab02/lab2_orders.csv]\n",
      "PushedFilters: [IsNotNull(order_id)]\n",
      "ReadSchema: struct<order_id:int,order_date:timestamp>\n",
      "\n",
      "(2) Filter\n",
      "Input [2]: [order_id#13, order_date#15]\n",
      "Condition : isnotnull(order_id#13)\n",
      "\n",
      "(3) BroadcastExchange\n",
      "Input [2]: [order_id#13, order_date#15]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1615]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [3]: [order_id#17, product_id#18, quantity#19]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab02/lab2_order_items.csv]\n",
      "PushedFilters: [IsNotNull(order_id), IsNotNull(product_id)]\n",
      "ReadSchema: struct<order_id:int,product_id:int,quantity:int>\n",
      "\n",
      "(5) Filter\n",
      "Input [3]: [order_id#17, product_id#18, quantity#19]\n",
      "Condition : (isnotnull(order_id#17) AND isnotnull(product_id#18))\n",
      "\n",
      "(6) BroadcastHashJoin\n",
      "Left keys [1]: [order_id#13]\n",
      "Right keys [1]: [order_id#17]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(7) Project\n",
      "Output [3]: [order_date#15, product_id#18, quantity#19]\n",
      "Input [5]: [order_id#13, order_date#15, order_id#17, product_id#18, quantity#19]\n",
      "\n",
      "(8) Scan csv \n",
      "Output [2]: [product_id#8, price#12]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab02/lab2_products.csv]\n",
      "PushedFilters: [IsNotNull(product_id)]\n",
      "ReadSchema: struct<product_id:int,price:double>\n",
      "\n",
      "(9) Filter\n",
      "Input [2]: [product_id#8, price#12]\n",
      "Condition : isnotnull(product_id#8)\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [2]: [product_id#8, price#12]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1619]\n",
      "\n",
      "(11) BroadcastHashJoin\n",
      "Left keys [1]: [product_id#18]\n",
      "Right keys [1]: [product_id#8]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(12) Project\n",
      "Output [3]: [quantity#19, price#12, cast(order_date#15 as date) AS _groupingexpression#214]\n",
      "Input [5]: [order_date#15, product_id#18, quantity#19, product_id#8, price#12]\n",
      "\n",
      "(13) HashAggregate\n",
      "Input [3]: [quantity#19, price#12, _groupingexpression#214]\n",
      "Keys [1]: [_groupingexpression#214]\n",
      "Functions [1]: [partial_sum((cast(quantity#19 as double) * price#12))]\n",
      "Aggregate Attributes [1]: [sum#215]\n",
      "Results [2]: [_groupingexpression#214, sum#216]\n",
      "\n",
      "(14) Exchange\n",
      "Input [2]: [_groupingexpression#214, sum#216]\n",
      "Arguments: hashpartitioning(_groupingexpression#214, 200), ENSURE_REQUIREMENTS, [plan_id=1624]\n",
      "\n",
      "(15) HashAggregate\n",
      "Input [2]: [_groupingexpression#214, sum#216]\n",
      "Keys [1]: [_groupingexpression#214]\n",
      "Functions [1]: [sum((cast(quantity#19 as double) * price#12))]\n",
      "Aggregate Attributes [1]: [sum((cast(quantity#19 as double) * price#12))#213]\n",
      "Results [2]: [_groupingexpression#214 AS d#206, sum((cast(quantity#19 as double) * price#12))#213 AS gmv#207]\n",
      "\n",
      "(16) AdaptiveSparkPlan\n",
      "Output [2]: [d#206, gmv#207]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Record Spark UI metrics for both runs in lab2_metrics_log.csv\n"
     ]
    }
   ],
   "source": [
    "# Case B: project early\n",
    "b = (orders.select(\"order_id\",\"order_date\")\n",
    "            .join(order_items.select(\"order_id\",\"product_id\",\"quantity\"), \"order_id\")\n",
    "            .join(products.select(\"product_id\",\"price\"), \"product_id\")\n",
    "            .groupBy(F.to_date(\"order_date\").alias(\"d\"))\n",
    "            .agg(F.sum(F.col(\"quantity\")*F.col(\"price\")).alias(\"gmv\")))\n",
    "b.explain(\"formatted\")\n",
    "_ = b.count()\n",
    "\n",
    "print(\"Record Spark UI metrics for both runs in lab2_metrics_log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970fd77",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6e6131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551db31-f7bc-418f-a584-02903d37ff40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
