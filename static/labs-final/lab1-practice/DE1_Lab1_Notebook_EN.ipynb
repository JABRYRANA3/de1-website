{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca9dd67",
   "metadata": {},
   "source": [
    "# DE1 — Lab 1: PySpark Warmup and Reading Plans\n",
    "> Author : Badr TAJINI - Data Engineering I - ESIEE 2025-2026\n",
    "---\n",
    "\n",
    "This notebook is the **student deliverable**. Execute all cells and attach evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9c7e5",
   "metadata": {},
   "source": [
    "## 0. Imports and Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6065897f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/20 20:32:49 WARN Utils: Your hostname, Rana, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/20 20:32:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/20 20:32:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "import os, sys, datetime, pathlib\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "print(\"Python:\", sys.version)\n",
    "spark = SparkSession.builder.appName(\"de1-lab1\").getOrCreate()\n",
    "print(\"Spark:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a14316",
   "metadata": {},
   "source": [
    "## 1. Load the CSV inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1d81df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2700\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+---+-----------+-----+----------------------------------------------------------------------------+\n",
      "|id |category   |value|text                                                                        |\n",
      "+---+-----------+-----+----------------------------------------------------------------------------+\n",
      "|0  |toys       |48.47|metrics ui data elt row columnar reduce warehouse shuffle join spark elt    |\n",
      "|1  |books      |39.9 |metrics row lake aggregate columnar data reduce row columnar filter         |\n",
      "|2  |grocery    |7.96 |lake join partition scala elt data                                          |\n",
      "|3  |electronics|5.15 |spark scala elt filter join columnar lake lake plan warehouse columnar spark|\n",
      "|4  |toys       |44.87|aggregate metrics row row filter lake map metrics columnar spark            |\n",
      "+---+-----------+-----+----------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "src_a = \"lab1_dataset_a.csv\"\n",
    "src_b = \"lab1_dataset_b.csv\"\n",
    "df_a = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src_a)\n",
    "df_b = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src_b)\n",
    "df = df_a.unionByName(df_b)\n",
    "df.cache()\n",
    "print(\"Rows:\", df.count())\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de95760a",
   "metadata": {},
   "source": [
    "## 2. Top‑N with **RDD** API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3339fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lake', 1215),\n",
       " ('scala', 1200),\n",
       " ('elt', 1199),\n",
       " ('metrics', 1190),\n",
       " ('row', 1183),\n",
       " ('join', 1169),\n",
       " ('warehouse', 1168),\n",
       " ('shuffle', 1160),\n",
       " ('ui', 1145),\n",
       " ('aggregate', 1144)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD pipeline: tokenize 'text' column and count tokens\n",
    "rdd = df.select(\"text\").rdd.flatMap(lambda row: (row[0] or \"\").lower().split())\n",
    "pair = rdd.map(lambda t: (t, 1))\n",
    "counts = pair.reduceByKey(lambda a,b: a+b)\n",
    "top_rdd = counts.sortBy(lambda kv: (-kv[1], kv[0])).take(10)\n",
    "top_rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5307d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote outputs/top10_rdd.csv\n"
     ]
    }
   ],
   "source": [
    "# Save as CSV (token,count)\n",
    "pathlib.Path(\"outputs\").mkdir(exist_ok=True)\n",
    "with open(\"outputs/top10_rdd.csv\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"token,count\\n\")\n",
    "    for t,c in top_rdd:\n",
    "        f.write(f\"{t},{c}\\n\")\n",
    "print(\"Wrote outputs/top10_rdd.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f557e1ce-9f3b-4744-afa7-6ea18902c045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"text\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f6217",
   "metadata": {},
   "source": [
    "### RDD plan — evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d76c5270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved proof/plan_rdd.txt\n"
     ]
    }
   ],
   "source": [
    "# Trigger an action and record a textual plan for evidence\n",
    "_ = counts.count()\n",
    "plan_rdd = df._jdf.queryExecution().executedPlan().toString()\n",
    "pathlib.Path(\"proof\").mkdir(exist_ok=True)\n",
    "with open(\"proof/plan_rdd.txt\",\"w\") as f:\n",
    "    f.write(str(datetime.datetime.now()) + \"\\n\\n\")\n",
    "    f.write(plan_rdd)\n",
    "print(\"Saved proof/plan_rdd.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7212d",
   "metadata": {},
   "source": [
    "## 3. Top‑N with **DataFrame** API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70d62ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|token    |count|\n",
      "+---------+-----+\n",
      "|lake     |1215 |\n",
      "|scala    |1200 |\n",
      "|elt      |1199 |\n",
      "|metrics  |1190 |\n",
      "|row      |1183 |\n",
      "|join     |1169 |\n",
      "|warehouse|1168 |\n",
      "|shuffle  |1160 |\n",
      "|ui       |1145 |\n",
      "|aggregate|1144 |\n",
      "+---------+-----+\n",
      "\n",
      "Wrote outputs/top10_df.csv\n"
     ]
    }
   ],
   "source": [
    "tokens = F.explode(F.split(F.lower(F.col(\"text\")), \"\\\\s+\")).alias(\"token\")\n",
    "df_tokens = df.select(tokens).where(F.col(\"token\") != \"\")\n",
    "agg_df = df_tokens.groupBy(\"token\").agg(F.count(\"*\").alias(\"count\"))\n",
    "top_df = agg_df.orderBy(F.desc(\"count\"), F.asc(\"token\")).limit(10)\n",
    "top_df.show(truncate=False)\n",
    "top_df.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"outputs/top10_df_tmp\")\n",
    "# move single part file to stable path\n",
    "import glob, shutil\n",
    "part = glob.glob(\"outputs/top10_df_tmp/part*\")[0]\n",
    "shutil.copy(part, \"outputs/top10_df.csv\")\n",
    "print(\"Wrote outputs/top10_df.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463644ee",
   "metadata": {},
   "source": [
    "### DataFrame plan — evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "992fa16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved proof/plan_df.txt\n"
     ]
    }
   ],
   "source": [
    "plan_df = top_df._jdf.queryExecution().executedPlan().toString()\n",
    "with open(\"proof/plan_df.txt\",\"w\") as f:\n",
    "    f.write(str(datetime.datetime.now()) + \"\\n\\n\")\n",
    "    f.write(plan_df)\n",
    "print(\"Saved proof/plan_df.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31e21b2",
   "metadata": {},
   "source": [
    "## 4. Projection experiment: `select(\"*\")` vs minimal projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567aaf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (9)\n",
      "+- HashAggregate (8)\n",
      "   +- Exchange (7)\n",
      "      +- HashAggregate (6)\n",
      "         +- InMemoryTableScan (1)\n",
      "               +- InMemoryRelation (2)\n",
      "                     +- Union (5)\n",
      "                        :- Scan csv  (3)\n",
      "                        +- Scan csv  (4)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [2]: [category#18, value#19]\n",
      "Arguments: [category#18, value#19]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [id#17, category#18, value#19, text#20], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "\n",
      "(3) Scan csv \n",
      "Output [4]: [id#17, category#18, value#19, text#20]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab01/lab1_dataset_a.csv]\n",
      "ReadSchema: struct<id:int,category:string,value:double,text:string>\n",
      "\n",
      "(4) Scan csv \n",
      "Output [4]: [id#38, category#39, value#40, text#41]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab01/lab1_dataset_b.csv]\n",
      "ReadSchema: struct<id:int,category:string,value:double,text:string>\n",
      "\n",
      "(5) Union\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [2]: [category#18, value#19]\n",
      "Keys [1]: [category#18]\n",
      "Functions [1]: [partial_sum(value#19)]\n",
      "Aggregate Attributes [1]: [sum#857]\n",
      "Results [2]: [category#18, sum#858]\n",
      "\n",
      "(7) Exchange\n",
      "Input [2]: [category#18, sum#858]\n",
      "Arguments: hashpartitioning(category#18, 200), ENSURE_REQUIREMENTS, [plan_id=439]\n",
      "\n",
      "(8) HashAggregate\n",
      "Input [2]: [category#18, sum#858]\n",
      "Keys [1]: [category#18]\n",
      "Functions [1]: [sum(value#19)]\n",
      "Aggregate Attributes [1]: [sum(value#19)#796]\n",
      "Results [2]: [category#18, sum(value#19)#796 AS sum_value#791]\n",
      "\n",
      "(9) AdaptiveSparkPlan\n",
      "Output [2]: [category#18, sum_value#791]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (9)\n",
      "+- HashAggregate (8)\n",
      "   +- Exchange (7)\n",
      "      +- HashAggregate (6)\n",
      "         +- InMemoryTableScan (1)\n",
      "               +- InMemoryRelation (2)\n",
      "                     +- Union (5)\n",
      "                        :- Scan csv  (3)\n",
      "                        +- Scan csv  (4)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [2]: [category#18, value#19]\n",
      "Arguments: [category#18, value#19]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [id#17, category#18, value#19, text#20], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "\n",
      "(3) Scan csv \n",
      "Output [4]: [id#17, category#18, value#19, text#20]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab01/lab1_dataset_a.csv]\n",
      "ReadSchema: struct<id:int,category:string,value:double,text:string>\n",
      "\n",
      "(4) Scan csv \n",
      "Output [4]: [id#38, category#39, value#40, text#41]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/mnt/c/Users/rania/OneDrive/Documents/Lab01/lab1_dataset_b.csv]\n",
      "ReadSchema: struct<id:int,category:string,value:double,text:string>\n",
      "\n",
      "(5) Union\n",
      "\n",
      "(6) HashAggregate\n",
      "Input [2]: [category#18, value#19]\n",
      "Keys [1]: [category#18]\n",
      "Functions [1]: [partial_sum(value#19)]\n",
      "Aggregate Attributes [1]: [sum#1029]\n",
      "Results [2]: [category#18, sum#1030]\n",
      "\n",
      "(7) Exchange\n",
      "Input [2]: [category#18, sum#1030]\n",
      "Arguments: hashpartitioning(category#18, 200), ENSURE_REQUIREMENTS, [plan_id=570]\n",
      "\n",
      "(8) HashAggregate\n",
      "Input [2]: [category#18, sum#1030]\n",
      "Keys [1]: [category#18]\n",
      "Functions [1]: [sum(value#19)]\n",
      "Aggregate Attributes [1]: [sum(value#19)#968]\n",
      "Results [2]: [category#18, sum(value#19)#968 AS sum_value#965]\n",
      "\n",
      "(9) AdaptiveSparkPlan\n",
      "Output [2]: [category#18, sum_value#965]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "Open Spark UI at http://localhost:4040 while each job runs and record metrics into lab1_metrics_log.csv\n"
     ]
    }
   ],
   "source": [
    "# Case A: select all columns then aggregate on 'category'\n",
    "all_cols = df.select(\"*\").groupBy(\"category\").agg(F.sum(\"value\").alias(\"sum_value\"))\n",
    "all_cols.explain(\"formatted\")\n",
    "_ = all_cols.count()  # trigger\n",
    "\n",
    "# Case B: minimal projection then aggregate\n",
    "proj = df.select(\"category\",\"value\").groupBy(\"category\").agg(F.sum(\"value\").alias(\"sum_value\"))\n",
    "proj.explain(\"formatted\")\n",
    "_ = proj.count()  # trigger\n",
    "\n",
    "print(\"Open Spark UI at http://localhost:4040 while each job runs and record metrics into lab1_metrics_log.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0053166",
   "metadata": {},
   "source": [
    "## 5. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "864557eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c93968-6c13-41e9-9ce4-d224f01d69cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
